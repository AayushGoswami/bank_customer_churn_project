{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Customer Churn Prediction: Model Training\n",
    "\n",
    "This notebook demonstrates the process of building a machine learning model to predict customer churn for a bank. The workflow includes:\n",
    "- Downloading and loading the dataset\n",
    "- Data preprocessing and feature engineering\n",
    "- Handling class imbalance with SMOTE\n",
    "- Training a Random Forest Classifier\n",
    "- Evaluating model performance\n",
    "- Saving the trained model for future use\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRcXD6Ojv1kJ",
    "outputId": "bce9fe62-9628-4c7f-c6ce-bb0abbb0161d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (1.6.1)\n",
      "Requirement already satisfied: imblearn in /usr/local/python/3.12.1/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (0.0)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: kaggle in /usr/local/python/3.12.1/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.7.4.2)\n",
      "Requirement already satisfied: kagglehub in /usr/local/python/3.12.1/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.3.12)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 1)) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/python/3.12.1/lib/python3.12/site-packages (from imblearn->-r requirements.txt (line 3)) (0.13.0)\n",
      "Requirement already satisfied: bleach in /home/codespace/.local/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 5)) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /home/codespace/.local/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 5)) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer in /home/codespace/.local/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/python/3.12.1/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 5)) (6.30.2)\n",
      "Requirement already satisfied: python-slugify in /usr/local/python/3.12.1/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 5)) (8.0.4)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 5)) (76.0.0)\n",
      "Requirement already satisfied: six>=1.10 in /home/codespace/.local/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /usr/local/python/3.12.1/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 5)) (1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /home/codespace/.local/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: webencodings in /home/codespace/.local/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 5)) (0.5.1)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from kagglehub->-r requirements.txt (line 6)) (24.2)\n",
      "Requirement already satisfied: pyyaml in /home/codespace/.local/lib/python3.12/site-packages (from kagglehub->-r requirements.txt (line 6)) (6.0.2)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from imbalanced-learn->imblearn->-r requirements.txt (line 3)) (0.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages from requirements.txt\n",
    "%pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download the Dataset\n",
    "\n",
    "We use the `kagglehub` library to download the latest version of the Bank Customer Churn Prediction dataset from Kaggle. Make sure you have the necessary API credentials set up for Kaggle access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chFQ0xrRwc47",
    "outputId": "879c6c78-fea7-4d13-d895-6f35243b10ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/shantanudhakadd/bank-customer-churn-prediction?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 262k/262k [00:00<00:00, 422kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /home/codespace/.cache/kagglehub/datasets/shantanudhakadd/bank-customer-churn-prediction/versions/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"shantanudhakadd/bank-customer-churn-prediction\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "We import essential libraries for data manipulation, model building, evaluation, and handling class imbalance. Notably, we use `pandas` for data handling, `scikit-learn` for machine learning, and `imblearn` for SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EV2dHO9D9Kmi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Inspect the Dataset\n",
    "\n",
    "We load the dataset into a pandas DataFrame and perform an initial inspection. Unnecessary columns such as identifiers and geographical information are dropped to focus on relevant features. We also apply one-hot encoding to categorical variables to prepare the data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qkm14g4ayclM",
    "outputId": "42016049-95e1-418d-98b9-aac48bb78a86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset loaded successfully ---\n",
      "Original DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "\n",
      "Original DataFrame Head:\n",
      "\n",
      "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
      "0          1    15634602  Hargrave          619    France  Female   42   \n",
      "1          2    15647311      Hill          608     Spain  Female   41   \n",
      "2          3    15619304      Onio          502    France  Female   42   \n",
      "3          4    15701354      Boni          699    France  Female   39   \n",
      "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
      "\n",
      "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "0       2       0.00              1          1               1   \n",
      "1       1   83807.86              1          0               1   \n",
      "2       8  159660.80              3          1               0   \n",
      "3       1       0.00              2          0               0   \n",
      "4       2  125510.82              1          1               1   \n",
      "\n",
      "   EstimatedSalary  Exited  \n",
      "0        101348.88       1  \n",
      "1        112542.58       0  \n",
      "2        113931.57       1  \n",
      "3         93826.63       0  \n",
      "4         79084.10       0  \n",
      "\n",
      "--- Dropped columns: ['RowNumber', 'CustomerId', 'Surname', 'Geography'] ---\n",
      "\n",
      "DataFrame Info after dropping columns:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CreditScore      10000 non-null  int64  \n",
      " 1   Gender           10000 non-null  object \n",
      " 2   Age              10000 non-null  int64  \n",
      " 3   Tenure           10000 non-null  int64  \n",
      " 4   Balance          10000 non-null  float64\n",
      " 5   NumOfProducts    10000 non-null  int64  \n",
      " 6   HasCrCard        10000 non-null  int64  \n",
      " 7   IsActiveMember   10000 non-null  int64  \n",
      " 8   EstimatedSalary  10000 non-null  float64\n",
      " 9   Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(7), object(1)\n",
      "memory usage: 781.4+ KB\n",
      "\n",
      "--- Applied One-Hot Encoding to Gender ---\n",
      "\n",
      "Processed DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CreditScore      10000 non-null  int64  \n",
      " 1   Age              10000 non-null  int64  \n",
      " 2   Tenure           10000 non-null  int64  \n",
      " 3   Balance          10000 non-null  float64\n",
      " 4   NumOfProducts    10000 non-null  int64  \n",
      " 5   HasCrCard        10000 non-null  int64  \n",
      " 6   IsActiveMember   10000 non-null  int64  \n",
      " 7   EstimatedSalary  10000 non-null  float64\n",
      " 8   Exited           10000 non-null  int64  \n",
      " 9   Gender_Male      10000 non-null  int64  \n",
      "dtypes: float64(2), int64(8)\n",
      "memory usage: 781.4 KB\n",
      "\n",
      "Processed DataFrame Head:\n",
      "   CreditScore  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
      "0          619   42       2       0.00              1          1   \n",
      "1          608   41       1   83807.86              1          0   \n",
      "2          502   42       8  159660.80              3          1   \n",
      "3          699   39       1       0.00              2          0   \n",
      "4          850   43       2  125510.82              1          1   \n",
      "\n",
      "   IsActiveMember  EstimatedSalary  Exited  Gender_Male  \n",
      "0               1        101348.88       1            0  \n",
      "1               1        112542.58       0            0  \n",
      "2               0        113931.57       1            0  \n",
      "3               0         93826.63       0            0  \n",
      "4               1         79084.10       0            0  \n"
     ]
    }
   ],
   "source": [
    "# Define the path to your CSV file\n",
    "file_path = path + '/Churn_Modelling.csv'\n",
    "\n",
    "# --- 1. Import the dataset into a pandas DataFrame ---\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"--- Dataset loaded successfully ---\")\n",
    "    print(\"Original DataFrame Info:\")\n",
    "    df.info()\n",
    "    print(\"\\nOriginal DataFrame Head:\\n\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {file_path}\")\n",
    "    # Exit or handle the error appropriately if the file isn't found\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Drop the unnecessary columns ---\n",
    "columns_to_drop = ['RowNumber', 'CustomerId', 'Surname', 'Geography']\n",
    "\n",
    "# Create a new dataframe to store the processed data\n",
    "df_processed = df.drop(columns=columns_to_drop)\n",
    "print(f\"\\n--- Dropped columns: {columns_to_drop} ---\\n\")\n",
    "print(\"DataFrame Info after dropping columns:\\n\")\n",
    "df_processed.info()\n",
    "\n",
    "# --- 3. Preprocess categorical features using One-Hot Encoding ---\n",
    "categorical_cols = ['Gender']\n",
    "\n",
    "# Use pandas get_dummies for easy One-Hot Encoding\n",
    "# drop_first=True helps avoid multicollinearity by removing one category per feature\n",
    "df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True, dtype=int) # Specify dtype=int for 0/1\n",
    "\n",
    "print(\"\\n--- Applied One-Hot Encoding to Gender ---\\n\")\n",
    "print(\"Processed DataFrame Info:\")\n",
    "df_processed.info()\n",
    "print(\"\\nProcessed DataFrame Head:\")\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Features and Target\n",
    "\n",
    "We separate the features (`X`) from the target variable (`y`). The target variable, `Exited`, indicates whether a customer has churned (1) or not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pjPexuFa4Rvv",
    "outputId": "55ba00f6-e8f0-4a53-94e9-6837642df77a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Split data into features (X) and target (y) ---\n",
      "Features (X) shape: (10000, 9)\n",
      "Target (y) shape: (10000,)\n",
      "\n",
      "Features (X) Head:\n",
      "   CreditScore  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
      "0          619   42       2       0.00              1          1   \n",
      "1          608   41       1   83807.86              1          0   \n",
      "2          502   42       8  159660.80              3          1   \n",
      "3          699   39       1       0.00              2          0   \n",
      "4          850   43       2  125510.82              1          1   \n",
      "\n",
      "   IsActiveMember  EstimatedSalary  Gender_Male  \n",
      "0               1        101348.88            0  \n",
      "1               1        112542.58            0  \n",
      "2               0        113931.57            0  \n",
      "3               0         93826.63            0  \n",
      "4               1         79084.10            0  \n",
      "\n",
      "Target (y) Head:\n",
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: Exited, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Split the data into features (X) and target (y) ---\n",
    "# The target variable is 'Exited', which indicates whether a customer churned (1) or not (0)\n",
    "X = df_processed.drop('Exited', axis=1) # Features are all columns except 'Exited'\n",
    "y = df_processed['Exited'] # Target variable is 'Exited'\n",
    "\n",
    "print(\"\\n--- Split data into features (X) and target (y) ---\")\n",
    "print(\"Features (X) shape:\", X.shape)\n",
    "print(\"Target (y) shape:\", y.shape)\n",
    "print(\"\\nFeatures (X) Head:\")\n",
    "print(X.head())\n",
    "print(\"\\nTarget (y) Head:\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split and Addressing Class Imbalance\n",
    "\n",
    "We split the data into training and testing sets, maintaining the distribution of the target variable using stratification. To address class imbalance, we apply SMOTE (Synthetic Minority Over-sampling Technique) to both training and testing sets, ensuring balanced classes for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xCf_wolv4Z9W",
    "outputId": "68416ad5-cdce-484c-ca0e-7d720d5187e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Split data into training and testing sets ---\n",
      "X_train shape (before SMOTE): (8000, 9)\n",
      "X_test shape: (2000, 9)\n",
      "y_train shape (before SMOTE): (8000,)\n",
      "y_test shape: (2000,)\n",
      "Value counts of y_train before SMOTE:\n",
      " Exited\n",
      "0    6370\n",
      "1    1630\n",
      "Name: count, dtype: int64\n",
      "Value counts of y_test before SMOTE:\n",
      " Exited\n",
      "0    1593\n",
      "1     407\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Applying SMOTE to the training and testing data ---\n",
      "X_train shape (after SMOTE): (12740, 9)\n",
      "y_train shape (after SMOTE): (12740,)\n",
      "Value counts of y_train after SMOTE:\n",
      " Exited\n",
      "1    6370\n",
      "0    6370\n",
      "Name: count, dtype: int64\n",
      "X_train shape (after SMOTE): (3186, 9)\n",
      "y_train shape (after SMOTE): (3186,)\n",
      "Value counts of y_test after SMOTE:\n",
      " Exited\n",
      "0    1593\n",
      "1    1593\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Split the data into training and testing sets ---\n",
    "# We'll use 80% of the data for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Stratify to maintain target distribution\n",
    "\n",
    "print(\"\\n--- Split data into training and testing sets ---\")\n",
    "print(\"X_train shape (before SMOTE):\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape (before SMOTE):\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"Value counts of y_train before SMOTE:\\n\", y_train.value_counts())\n",
    "print(\"Value counts of y_test before SMOTE:\\n\", y_test.value_counts())\n",
    "# --- 6. Apply SMOTE to the training data ---\n",
    "print(\"\\n--- Applying SMOTE to the training and testing data ---\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "print(\"X_train shape (after SMOTE):\", X_train_res.shape)\n",
    "print(\"y_train shape (after SMOTE):\", y_train_res.shape)\n",
    "print(\"Value counts of y_train after SMOTE:\\n\", y_train_res.value_counts())\n",
    "\n",
    "X_test_res, y_test_res = smote.fit_resample(X_test, y_test)\n",
    "print(\"X_train shape (after SMOTE):\", X_test_res.shape)\n",
    "print(\"y_train shape (after SMOTE):\", y_test_res.shape)\n",
    "print(\"Value counts of y_test after SMOTE:\\n\", y_test_res.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (Optional) Feature Scaling\n",
    "\n",
    "Feature scaling is often applied to numerical features to standardize their ranges. Although this step is currently commented out, it can be enabled if needed for algorithms sensitive to feature scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaDes1A67Sq6"
   },
   "outputs": [],
   "source": [
    "# # --- Scale numerical features ---\n",
    "# # Original numerical columns: 'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary'\n",
    "# numerical_cols_to_scale = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "# # Note: 'HasCrCard' and 'IsActiveMember' are binary but often included in scaling.\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Fit the scaler on the training data and transform it\n",
    "# X_train[numerical_cols_to_scale] = scaler.fit_transform(X_train[numerical_cols_to_scale])\n",
    "\n",
    "# # Transform the test data using the *same* scaler fitted on the training data\n",
    "# X_test[numerical_cols_to_scale] = scaler.transform(X_test[numerical_cols_to_scale])\n",
    "\n",
    "# print(\"\\n--- Scaled numerical features ---\")\n",
    "# print(\"X_train Head after scaling:\")\n",
    "# print(X_train.head())\n",
    "# print(\"\\nX_test Head after scaling:\")\n",
    "# print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train a Random Forest Classifier\n",
    "\n",
    "We initialize and train a Random Forest Classifier using the SMOTE-balanced training data. Random Forest is an ensemble method that builds multiple decision trees and aggregates their predictions for improved accuracy and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkgUSdBS7do3",
    "outputId": "2bd660cb-d40f-4363-91f9-cc9419d8b369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Random Forest Classifier ---\n",
      "--- Random Forest Classifier trained successfully ---\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Train a Random Forest Classifier model ---\n",
    "print(\"\\n--- Training Random Forest Classifier ---\")\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "# n_estimators: The number of trees in the forest.\n",
    "# random_state: Ensures reproducibility.\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(X_train_res, y_train_res)\n",
    "print(\"--- Random Forest Classifier trained successfully ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Make Predictions\n",
    "\n",
    "We use the trained Random Forest model to make predictions on the SMOTE-balanced test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMX4Ppuc75kb",
    "outputId": "a5d3f883-3844-4cb3-8562-6776385d146f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Making predictions on the test set ---\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Make predictions on the test data ---\n",
    "print(\"\\n--- Making predictions on the test set ---\")\n",
    "y_pred = model.predict(X_test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model Performance\n",
    "\n",
    "We evaluate the model using accuracy, classification report (precision, recall, F1-score), and confusion matrix. These metrics provide insights into the model's predictive performance and its ability to distinguish between churned and non-churned customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aC9kdeY8FWB",
    "outputId": "9d19b8b6-ddb8-4e15-f5c7-fa0ca1393822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating the model ---\n",
      "Accuracy: 0.8214\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.83      1593\n",
      "           1       0.84      0.79      0.82      1593\n",
      "\n",
      "    accuracy                           0.82      3186\n",
      "   macro avg       0.82      0.82      0.82      3186\n",
      "weighted avg       0.82      0.82      0.82      3186\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1353  240]\n",
      " [ 329 1264]]\n"
     ]
    }
   ],
   "source": [
    "# --- 9. Evaluate the model ---\n",
    "print(\"\\n--- Evaluating the model ---\")\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test_res, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print Classification Report (includes precision, recall, f1-score)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_res, y_pred))\n",
    "\n",
    "# Print Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_res, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the Trained Model\n",
    "\n",
    "Finally, we save the trained model to disk using `joblib`. This allows us to reuse the model for future predictions without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZlETpFbQ8Kfg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving the trained model ---\n",
      "Created directory: model\n",
      "Model successfully saved to model/Bank_Churn_pred_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "\n",
    "print(\"\\n--- Saving the trained model ---\")\n",
    "\n",
    "model_dir = 'model'\n",
    "model_filename = 'Bank_Churn_pred_model.joblib'\n",
    "model_path = os.path.join(model_dir, model_filename)\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    print(f\"Created directory: {model_dir}\")\n",
    "\n",
    "try:\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Model successfully saved to {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the model: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
